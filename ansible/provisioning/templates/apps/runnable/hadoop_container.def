Bootstrap: localimage
From: apps/base/{{ app_base_image_file }}

%files
    apps/{{ app_name }} /opt
    #{{ installation_path }}/apps/hadoop/files_dir /opt/files_dir
    #{{ installation_path }}/apps/hadoop/setup.sh /opt/setup.sh
    #{{ installation_path }}/apps/hadoop/java_snitch.sh /opt/java_snitch.sh
    # To avoid repeating hadoop download
    #"{{ installation_path }}/apps/hadoop/hadoop-{{ hadoop_version }}.tar.gz" /opt/hadoop-{{ hadoop_version }}.tar.gz
    # To avoid repeating spark download
    #"{{ installation_path }}/apps/hadoop/spark-{{ spark_version }}-bin-without-hadoop.tgz" /opt/spark-{{ spark_version }}-bin-without-hadoop.tgz

%post
    # Install basic software and Java
    apt-get -y update
    apt-get install -y wget gettext-base openssh-client openssh-server
    apt-get install -y openjdk-8-jdk

    # Trying to setup firewall (iptables prevent hadoop cluster from working)
    #apt-get remove -y iptables
    #apt-get install -y ufw kmod
    #apt-get install -y nftables
    #apt-get install -y --reinstall linux-headers-$(uname -r)
    #apt-get -y update && apt-get -y upgrade
    #modprobe iptable_filter
    #modprobe /lib/modules/$(uname -r)/kernel/net/ipv4/netfilter/iptable_filter.ko
    #ufw --force enable

    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

    # Setup SSH
    mkdir /run/sshd
    chmod 0711 /run/sshd

    # Setup hadoop
    cd /opt

    export HADOOP_CONF_DIR=/opt/files_dir/hadoop_conf
    export DATA_DIR="{{ bind_dir_on_container }}/files_dir/data_dir"
    mkdir -p files_dir/data_dir
    wget https://archive.apache.org/dist/hadoop/core/hadoop-{{ hadoop_version }}/hadoop-{{ hadoop_version }}.tar.gz
    mkdir hadoop && tar xf hadoop-{{ hadoop_version }}.tar.gz -C hadoop --strip-components 1
    chown -R $(whoami):$(whoami) hadoop
    cp files_dir/config/format_filesystem.sh hadoop
    mkdir -p $HADOOP_CONF_DIR && cp -r hadoop/etc/hadoop/* $HADOOP_CONF_DIR/
    envsubst < files_dir/config/hdfs-site.xml > $HADOOP_CONF_DIR/hdfs-site.xml
    cp files_dir/config/core-site.xml $HADOOP_CONF_DIR/core-site.xml
    cp files_dir/config/mapred-site.xml $HADOOP_CONF_DIR/mapred-site.xml
    cp files_dir/config/yarn-site.xml $HADOOP_CONF_DIR/yarn-site.xml
    cp files_dir/config/workers $HADOOP_CONF_DIR/workers

    # Setup Spark
    export SPARK_CONF_DIR=/opt/files_dir/spark_conf
    wget https://dlcdn.apache.org/spark/spark-{{ spark_version }}/spark-{{ spark_version }}-bin-without-hadoop.tgz
    mkdir spark && tar xf "spark-{{ spark_version }}-bin-without-hadoop.tgz" -C spark --strip-components 1
    chown -R $(whoami):$(whoami) spark
    mkdir -p $SPARK_CONF_DIR && cp -r spark/conf/* $SPARK_CONF_DIR/

%environment
    export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
    export JAVA_MAPPINGS_FOLDER_PATH="{{ bind_dir_on_container }}/files_dir/java_mappings"
    # Hadoop
    export DATA_DIR="{{ bind_dir_on_container }}/files_dir/data_dir"
    export HADOOP_HOME=/opt/hadoop
    export HADOOP_CONF_DIR="{{ bind_dir_on_container }}/files_dir/hadoop_conf"
    #export YARN_CONF_DIR=$HADOOP_CONF_DIR
    export HADOOP_LOG_DIR="{{ bind_dir_on_container }}/hadoop_logs"
    #export YARN_LOG_DIR=$HADOOP_LOG_DIR
    ## Hadoop users (hadoop 3.3.5)
    export HDFS_NAMENODE_USER=$(whoami)
    export HDFS_DATANODE_USER=$(whoami)
    export HDFS_SECONDARYNAMENODE_USER=$(whoami)
    export YARN_RESOURCEMANAGER_USER=$(whoami)
    export YARN_NODEMANAGER_USER=$(whoami)
    # Spark
    export SPARK_HOME=/opt/spark
    export SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath)
    export SPARK_CONF_DIR="{{ bind_dir_on_container }}/files_dir/spark_conf"

%startscript
    #cd /opt/BDWatchdog/MetricsFeeder && bash scripts/run_atop_stream_tmux.sh && \
    cd /opt/BDWatchdog/MetricsFeeder && bash scripts/run_atop_stream_with_java_translation.sh && bash /opt/java_snitch.sh && \
    bash /opt/setup.sh
